LLM — большая языковая модель, обученная предсказывать следующий токен.

Авторегрессия — генерация по одному токену вперед.

Трансформер — архитектура с attention.

Self-Attention — позволяет учитывать все токены контекста.

Контекстное окно — максимальное число токенов, которое модель может учитывать.

Логиты — необработанные выходы модели перед softmax.

Sampling:
  – top-k
  – top-p
  – temperature

Inference — выполнение модели на входе.

Latency — время отклика модели.

Availability — доступность сервиса.

KV-cache — кеш ключей/значений слоёв attention, ускоряет генерацию.

RAG — retrieval + генерация.

MoE — mixture-of-experts, sparse-архитектура.

Квантизация — уменьшение разрядности весов.

Distillation — перенос знаний от большой модели к маленькой.

Pruning — удаление малозначащих весов.

Fine-tuning:
  – full fine-tuning
  – LoRA
  – QLoRA

RLHF — обучение с обратной связью от человека.

Observability — мониторинг:
  – задержки
  – токены в секунду
  – распределение ошибок
  – красные флаги галлюцинаций
